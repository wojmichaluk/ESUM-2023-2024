{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjLwr7LHju9z"
   },
   "source": [
    "# Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y0Y7HX_yju91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "np.random.seed(1234)\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    '''\n",
    "    Convert label indices into one-hot vectors\n",
    "    \n",
    "    Args:\n",
    "        labels: n-element array with label indices.\n",
    "    \n",
    "    Returns:\n",
    "        n times k matrix where each row is a one-hot encoding\n",
    "        of a class label. k - number of classes.\n",
    "    '''\n",
    "    one_hot = np.zeros(shape = (labels.shape[0], np.max(labels) + 1))\n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    \n",
    "    return one_hot.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_params(ax, W):\n",
    "    '''\n",
    "    Draw parameters of a multinomial logistic regression in a tiles-like plot.\n",
    "    \n",
    "    Args:\n",
    "        ax:    Axis for plotting.\n",
    "        W:     Parameters of a multinomial logistic regression model.\n",
    "               Shape (d+1) times k, where k is the number of classes and\n",
    "               d is the number of explanatory variables.\n",
    "    '''\n",
    "    F = np.reshape(W[:-1, :].T, newshape=(2, -1, 28, 28))\n",
    "    tiles(ax, F)\n",
    "\n",
    "\n",
    "def tiles(ax, M):\n",
    "    '''\n",
    "    Draw volume M in a tiles-like plot.\n",
    "    \n",
    "    Args:\n",
    "        ax:    Axis for plotting.\n",
    "        M:     Volume to plot. Shape: r times c times h times w, where\n",
    "               r is the number of rows in the output plot,\n",
    "               c is the number of columns in the output plot,\n",
    "               h is the height of a single tile,\n",
    "               w is the width of a single tile.\n",
    "    '''\n",
    "    rows_count = M.shape[0]\n",
    "    cols_count = M.shape[1]\n",
    "    tile_height = M.shape[2]\n",
    "    tile_width = M.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(M[r, c])\n",
    "            ex_max = np.max(M[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (M[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    ax.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "\n",
    "In this lab we will use [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/). Lets import and prepare this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(plt.gca(), digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reshape each $28 \\times 28$ pixel image into a $1 \\times 784$ vector and append a fixed $1$ at the end. We will also convert images from $255$ from 255255 gray levels to $\\langle 0, 1 \\rangle$ interval. Finally, we will pick a random subset of 20,000 examples for training. The test part of the MNIST dataset will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Train set #####\n",
    "\n",
    "DATASET_SIZE = 20000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = np.reshape(mnist_train_images[:DATASET_SIZE],\n",
    "                                newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "\n",
    "ones = np.ones((mnist_train_images.shape[0], 1))\n",
    "mnist_train_images = np.concatenate((mnist_train_images, ones), axis=1)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "##### Validation set #####\n",
    "\n",
    "mnist_val_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_val_images = np.reshape(mnist_val_images, newshape=(-1, DIGIT_SIZE*DIGIT_SIZE))\n",
    "\n",
    "ones = np.ones((mnist_val_images.shape[0], 1))\n",
    "mnist_val_images = np.concatenate((mnist_val_images, ones), axis=1)\n",
    "\n",
    "mnist_val_labels = mnist.test_labels()\n",
    "mnist_val_labels = one_hot_encode(mnist_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression model\n",
    "\n",
    "Our goal is to fit a multinomial logistic regression model to the MNIST dataset. We start by implementing the link function in this model.\n",
    "\n",
    "### Softmax link function\n",
    "\n",
    "Implement `softmax` function which takes an $n \\times k$ matrix $\\mathbf{Z}$ with class logits and returns an $n \\times k$ matrix with categorical probability distributions for observations in $\\mathbf{Z}$. The dimensions are: $n$ - number of observations, $k$ - number of classes.\n",
    "\n",
    "---\n",
    "\n",
    "**Implementation note**. Imagine that $z_{ij} = 100$. Then in a typical floating-point precision we have:\n",
    "\n",
    "$$\\large\n",
    "p_{ij} = \\frac{\\mathrm{e}^{z_{ij}}}{\\sum_{l=1}^k \\mathrm{e}^{z_{il}}}\n",
    "       = \\frac{\\mathrm{e}^{z_{ij}}}{\\mathrm{e}^{z_{ij}} + \\sum_{\\substack{l=1 \\\\ l \\neq j}}^k \\mathrm{e}^{z_{il}}}\n",
    "       = \\frac{\\mathrm{e}^{100}}{\\mathrm{e}^{100} + \\sum_{\\substack{l=1 \\\\ l \\neq j}}^k \\mathrm{e}^{z_{il}}}\n",
    "       = \\frac{\\mathrm{inf}}{\\mathrm{inf}} = \\mathrm{nan}\n",
    "$$\n",
    "\n",
    "However, note that for every finite $u \\in \\mathbb{R}$ we have:\n",
    "\n",
    "$$\\large\n",
    "p_{ij} = \\frac{\\mathrm{e}^{z_{ij}}}{\\sum_{l=1}^k \\mathrm{e}^{z_{il}}}\n",
    "       = \\frac{\\mathrm{e}^{-u} \\mathrm{e}^{z_{ij}}}{\\mathrm{e}^{-u} \\sum_{l=1}^k \\mathrm{e}^{z_{il}}}\n",
    "       = \\frac{\\mathrm{e}^{z_{ij} - u}}{\\sum_{l=1}^k \\mathrm{e}^{z_{il} - u}}\n",
    "$$\n",
    "\n",
    "So to implement `softmax` in a numerically stable manner, we can simply shift the logits (independently for each observation) so that: $\\mathop{\\mathrm{max}}_{l = 1,\\ldots, k} z_{il} = 0$. That is, we set:\n",
    "\n",
    "$$\\large\n",
    "z_{ij} \\leftarrow z_{ij} - \\mathop{\\mathrm{max}}_{l = 1,\\ldots, k} z_{il}\n",
    "$$\n",
    "\n",
    "This basically turns an overflow issue into an underflow that we can disregard. Use this trick when implementing `softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    raise Exception('Implement softmax function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Next, we need to implement a cost function for our model and its gradient w.r.t the model parameters. Note that the cross-entropy cost increases proportionally to the number of input observations $n$. This is undesirable from an optimization point of view: the learning rate in gradient descent needs to be adjusted to the number of training observations. To avoid that issue we will calculate an <u>average</u> cross-entropy cost per input observation. That is, we will divide the cross-entropy cost (and therefore also its gradient) by $n$.\n",
    "\n",
    "---\n",
    "\n",
    "Implement `xentropy` function which takes as an input:\n",
    "- An $n \\times k$ matrix $\\mathbf{S}$ with categorical probability distributions for $n$ input observations.\n",
    "- An $n \\times k$ matrix $\\mathbf{T}$ with one-hot encoded class labels for $n$ input observations.\n",
    "\n",
    "and returns an average (per input observation) cross-entropy between $\\mathbf{T}$ and $\\mathbf{S}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xentropy(S, T):\n",
    "    raise Exception('Implement xentropy function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `grad_xentropy` function which takes as an input:\n",
    "- An $n \\times (d+1)$ matrix $\\mathbf{X}$ with explanatory variables for $n$ input observations.\n",
    "- An $n \\times k$ matrix $\\mathbf{S}$ with categorical probability distributions for observations in $\\mathbf{X}$.\n",
    "- An $n \\times k$ matrix $\\mathbf{T}$ with one-hot encoded class labels for observations in $\\mathbf{X}$.\n",
    "\n",
    "and returns a gradient of the average (per input observation) cross-entropy between $\\mathbf{T}$ and $\\mathbf{S}$. We assume a standard multinomial logistic regression parametrization:\n",
    "\n",
    "\\begin{align} \\large\n",
    "\\mathbf{S}_{ij} & = \\frac{\\mathrm{e}^{z_{ij}}}{\\sum_{l=1}^k \\mathrm{e}^{z_{il}}}, \\\\[1em]\n",
    "\\left[z_{ij}\\right]_{n \\times k} & = \\mathbf{Z} = \\mathbf{XW},\n",
    "\\end{align}\n",
    "\n",
    "and return the gradient w.r.t the model parameters: $\\mathbf{W}_{\\left(d+1\\right) \\times k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_xentropy(X, S, T):\n",
    "    raise Exception('Implement grad_xentropy function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent training\n",
    "\n",
    "We are now ready to implement a maximum likelihood estimation for multinomial logistic regression model. We begin with few utility functions.\n",
    "\n",
    "Implement `classify` function which takes as an input\n",
    "- An $(d+1) \\times k$ matrix $\\mathbf{W}$ with parameters of a multinomial logistic regression model.\n",
    "- An $n \\times (d+1)$ matrix $\\mathbf{X}$ with explanatory variables for $n$ input observations.\n",
    "\n",
    "and returns an $n \\times k$ NumPy array with predicted class labels in one-hot encoding. We assume that the class predicted for a given observation is the one with the largest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(W, X):\n",
    "    raise Exception('Implement classify function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_acc(P, T):\n",
    "    '''\n",
    "    Calculate classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        P: predicted labels in one-hot encoding,\n",
    "           shape n times k.\n",
    "        T: true labels in one-hot encoding,\n",
    "           shape n times k.           \n",
    "    \n",
    "    Returns:\n",
    "        Percentage of correctly predicted labels.\n",
    "    '''\n",
    "    accuracy = np.sum(P * T) / P.shape[0]\n",
    "    return 100.0 * accuracy\n",
    "\n",
    "\n",
    "def print_log(step, cost, train_acc, val_acc):\n",
    "    '''\n",
    "    A utility function used to display the progress of gd_fit.\n",
    "    '''\n",
    "    log = 'Step {:3d}\\tcost value: {:5.2f},\\ttrain accuracy: {:5.2f},\\t' \\\n",
    "          'validation accuracy: {:5.2f}'\n",
    "    log = log.format(step, cost.item(), train_acc.item(), val_acc.item())\n",
    "    \n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will minimize the cost function with gradient descent. Complete the implementation of `gd_fit` function following comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd_fit(W0, X, T, X_val, T_val, lr=1.0, steps=100, log_every=5):\n",
    "    '''\n",
    "    Fit multinomial logistic regression model with gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        W0:        An array with initial parameter values, shape (d+1) times k.\n",
    "        X:         An array with explanatory variables for input (train) observations,\n",
    "                   shape n times (d+1).\n",
    "        T:         An array with one-hot encoded class labels for input (train) observations, \n",
    "                   shape n times k.\n",
    "        X_val:     An array with explanatory variables for validation observations,\n",
    "                   shape m times (d+1).\n",
    "        T_val:     An array with one-hot encoded class labels for validation observations, \n",
    "                   shape m times k.\n",
    "        lr:        Learning rate.\n",
    "        steps:     Number of gradient descent steps to perform.\n",
    "        log_every: Number of steps between progress logs.\n",
    "    \n",
    "    Returns:\n",
    "        An (d+1) times k NumPy array with fitted parameters.\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    W = np.copy(W0)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Assuming that current model parameters are in `W`, calculate the value of\n",
    "        # the cost function and store it in `cost_val` variable.\n",
    "        \n",
    "        raise Exception('Complete implementation of gd_fit funcion')\n",
    "        \n",
    "        # Next, calculate the gradient of the cost function w.r.t the parameters\n",
    "        # in `W`. Use this gradient matrix to update `W` (according to the gradient\n",
    "        # descent update rule).\n",
    "        \n",
    "        raise Exception('Complete implementation of gd_fit funcion')\n",
    "        \n",
    "        \n",
    "        \n",
    "        P_train = classify(W, X)\n",
    "        train_acc = calc_acc(P_train, T)\n",
    "        \n",
    "        P_val = classify(W, X_val)\n",
    "        val_acc = calc_acc(P_val, T_val)\n",
    "        \n",
    "        if step == 0 or (step + 1) % log_every == 0:\n",
    "            print_log(step+1, cost_val, train_acc, val_acc)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting multinomial logistic regression model to MNIST.\n",
    "\n",
    "Lets prepare and plot some initial parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.random.randn(DIGIT_SIZE*DIGIT_SIZE + 1, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "draw_params(plt.gca(), W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit multinomial logistic regression model to the MNIST dataset. Initially we do 200 gradient descent steps starting from `W0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gd_fit(W0,\n",
    "           mnist_train_images, mnist_train_labels,\n",
    "           mnist_val_images, mnist_val_labels,\n",
    "           lr=3.0, steps=200, log_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we continue with additional 300 steps using a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gd_fit(W,\n",
    "           mnist_train_images, mnist_train_labels,\n",
    "           mnist_val_images, mnist_val_labels,\n",
    "           lr=0.3, steps=300, log_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "draw_params(plt.gca(), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting regularized multinomial logistic regression model to MNIST.\n",
    "\n",
    "Cross entropy cost function is convex but it is <u>not</u> strictly convex. We can turn it into a strictly convex cost function by adding a regularization term that penalizes magnitudes of model parameters:\n",
    "\n",
    "$$ \\large\n",
    "\\mathcal{L}_R\\left(\\mathbf{W}\\right) = \\mathcal{L}\\left(\\mathbf{W}\\right) +\n",
    "                                       \\frac{\\lambda}{2}\\sum_{i=1}^{d+1}\\sum_{j=1}^k w_{ij}^2,\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}\\left(\\mathbf{W}\\right)$ is the cross entropy cost.\n",
    "\n",
    "This is the same regularization that we used in ridge regression, and it can be derived as a MAP estimate in a Bayesian logistic regression model with Gaussian prior on model parameters (we do not need a conjugate prior to find the cost function for a MAP estimate). Let's implement this variant of the logistic regression model.\n",
    "\n",
    "Complete the implementation of `gd_fit_reg` function following comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd_fit_reg(W0, X, T, X_val, T_val, l2=0.005, lr=1.0, steps=100, log_every=5):\n",
    "    '''\n",
    "    Fit multinomial logistic regression model with gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        W0:        An array with initial parameter values, shape (d+1) times k.\n",
    "        X:         An array with explanatory variables for input (train) observations,\n",
    "                   shape n times (d+1).\n",
    "        T:         An array with one-hot encoded class labels for input (train) observations, \n",
    "                   shape n times k.\n",
    "        X_val:     An array with explanatory variables for validation observations,\n",
    "                   shape m times (d+1).\n",
    "        T_val:     An array with one-hot encoded class labels for validation observations, \n",
    "                   shape m times k.\n",
    "        l2:        The regularization strength (\\lambda hyper-paramater).\n",
    "        lr:        Learning rate.\n",
    "        steps:     Number of gradient descent steps to perform.\n",
    "        log_every: Number of steps between progress logs.\n",
    "    \n",
    "    Returns:\n",
    "        An (d+1) times k NumPy array with fitted parameters.\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    W = np.copy(W0)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Assuming that current model parameters are in `W`, calculate the value of\n",
    "        # the regularized cost function and store it in `cost_val` variable.\n",
    "        \n",
    "        raise Exception('Complete implementation of gd_fit_reg funcion')\n",
    "        \n",
    "        # Next, calculate the gradient of the regularized cost function w.r.t the\n",
    "        # parameters in `W`. Use this gradient matrix to update `W` (according to\n",
    "        # the gradient descent update rule).\n",
    "        \n",
    "        raise Exception('Complete implementation of gd_fit_reg funcion')\n",
    "        \n",
    "        \n",
    "        \n",
    "        P_train = classify(W, X)\n",
    "        train_acc = calc_acc(P_train, T)\n",
    "        \n",
    "        P_val = classify(W, X_val)\n",
    "        val_acc = calc_acc(P_val, T_val)\n",
    "        \n",
    "        if step == 0 or (step + 1) % log_every == 0:\n",
    "            print_log(step+1, cost_val, train_acc, val_acc)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit regularized multinomial logistic regression model to the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_reg = gd_fit_reg(W0,\n",
    "                   mnist_train_images, mnist_train_labels,\n",
    "                   mnist_val_images, mnist_val_labels,\n",
    "                   l2=0.005, lr=3.0,\n",
    "                   steps=200, log_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_reg = gd_fit_reg(W_reg,\n",
    "                   mnist_train_images, mnist_train_labels,\n",
    "                   mnist_val_images, mnist_val_labels,\n",
    "                   l2=0.005, lr=0.3,\n",
    "                   steps=300, log_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the parameters of the regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "draw_params(plt.gca(), W_reg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Random.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
