{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjLwr7LHju9z"
   },
   "source": [
    "# Introduction to Tensorflow and Tensorflow Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y0Y7HX_yju91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "np.random.seed(1234)\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_posterior_samples(ax, w_samples, data_x, data_y, data_ys, fill=False, title=None):\n",
    "    '''\n",
    "    Plot samples from a Bayesian linear regression.\n",
    "    \n",
    "    Args:\n",
    "        ax:        Axis for plotting.\n",
    "        w_samples: Samples from the posterior over regression parameters\n",
    "        data_x:    Explanatory variables.\n",
    "        data_y:    Responses.\n",
    "        data_ys:   Uncertainty in data_y.\n",
    "        fill:      Whether to fill area between the first and the last sample.\n",
    "        title:     Plot title.\n",
    "    '''\n",
    "    ax.errorbar(data_x[:, 0], data_y[:, 0], data_ys, None, marker=\"o\", ls='', capsize=5)\n",
    "    ax.set_xlabel('x', fontsize='xx-large')\n",
    "    ax.set_ylabel('y', fontsize='xx-large')\n",
    "\n",
    "    xmin, xmax = np.min(data_x[:, 0]), np.max(data_x[:, 0])\n",
    "    X = np.array([[xmin, 1], [xmax, 1]])\n",
    "    \n",
    "    Y = X @ tf.transpose(w_samples)\n",
    "    Y = tf.transpose(Y)\n",
    "    for y in Y:\n",
    "        ax.plot(X[:, 0], y, marker='', lw=1.0, alpha=0.5, color='r');\n",
    "    \n",
    "    if fill:\n",
    "        plt.fill_between(X[:, 0], Y[0, :], Y[-1, :], alpha=0.3)\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will return to the linear regression example from Lab 3.\n",
    "\n",
    "Let's prepare this data - our setup is the same as in Lab3, except this time we will use TensorFlow (TF) and TensorFlow Probability (TFP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D.W. Hogg et al. Data analysis recipes: Fitting a model to data, https://arxiv.org/abs/1008.4686, 2010\n",
    "hogg_data = np.array([[201, 592, 61],\n",
    "                      [244, 401, 25],\n",
    "                      [47, 583, 38],\n",
    "                      [287, 402, 15],\n",
    "                      [203, 495, 21],\n",
    "                      [58, 173, 15],\n",
    "                      [210, 479, 27],\n",
    "                      [202, 504, 14],\n",
    "                      [198, 510, 30],\n",
    "                      [158, 416, 16],\n",
    "                      [165, 393, 14],\n",
    "                      [201, 442, 25],\n",
    "                      [157, 317, 52],\n",
    "                      [131, 311, 16],\n",
    "                      [166, 400, 34],\n",
    "                      [160, 337, 31],\n",
    "                      [186, 423, 42],\n",
    "                      [125, 334, 26],\n",
    "                      [218, 533, 16],\n",
    "                      [146, 344, 22]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hogg_x_npy, hogg_y_npy, hogg_ys_npy = hogg_data[:, 0], hogg_data[:, 1], hogg_data[:, 2]\n",
    "\n",
    "hogg_x_npy, hogg_y_npy = hogg_x_npy[:, None], hogg_y_npy[:, None]\n",
    "\n",
    "ones = np.ones((hogg_x_npy.shape[0], 1))\n",
    "hogg_x_npy = np.concatenate((hogg_x_npy, ones), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, TensorFlow works with tensors :)\n",
    "\n",
    "So we will encapsulate our input data in constant tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hogg_x = tf.constant(hogg_x_npy, dtype=tf.float32)\n",
    "hogg_y = tf.constant(hogg_y_npy, dtype=tf.float32)\n",
    "hogg_ys = tf.constant(hogg_ys_npy, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hogg_x[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "plt.errorbar(hogg_x[:, 0], hogg_y[:, 0], hogg_ys, None, marker=\"o\", ls='', capsize=5)\n",
    "plt.xlabel('x', fontsize='xx-large')\n",
    "plt.ylabel('y', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, functions that expects NumPy arrays works without issues with TF tensors. There are implicit conversions between NumPy arrays and TF tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our Bayesian linear regression model. Our goal now is to implement `bayes_lin_reg` in TensorFlow. So this time we assume that all input arguments are TF tensors and we expect to return posterior parameters as TF tensors.\n",
    "\n",
    "There are several useful hints to know:\n",
    "- Basic math operators (+, -, *, /, @) are overloaded and will work with tensors.\n",
    "- Linear algebra is provided by `tf.linalg` package.\n",
    "- TF tensors do not implement a transpose property. So if `X` is a TF tensor then `X.T` construction won't work. You must use `tf.transpose(X)` instead.\n",
    "\n",
    "---\n",
    "\n",
    "Implement `bayes_lin_reg` function, which:\n",
    "- takes as an input:\n",
    "    - prior mean and covariance of the regression parameters $\\mathbf{w}$,\n",
    "    - response uncertainty ($\\sigma$ hyperparameter),\n",
    "    - matrix of explanatory variables and vector of responses,\n",
    "- and returns parameters of the posterior distribution over $\\mathbf{w}$ (mean and covariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_lin_reg(prior_mu, prior_Sigma, sigma, X, y):\n",
    "    raise Exception('Unimplemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `bayes_lin_reg` implemented, we can move to the prior for $\\mathbf{w}$. We will use the same prior as in Lab 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mean, y_var = np.mean(hogg_y[:, 0]), np.var(hogg_y[:, 0])\n",
    "\n",
    "mu_0 = np.array([0, y_mean]).reshape(-1, 1)\n",
    "Sigma_0 = np.array([[1, 0],\n",
    "                    [0, y_var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember that we work with tensors now. Prior parameters do not change, so we can make them constant tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mean, y_var = tf.constant(y_mean, dtype=tf.float32), tf.constant(y_var, dtype=tf.float32)\n",
    "mu_0, Sigma_0 = tf.constant(mu_0, dtype=tf.float32), tf.constant(Sigma_0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the posterior parameters (assuming $\\sigma=25$) and see if our implementation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 25.\n",
    "\n",
    "mu_n, Sigma_n = bayes_lin_reg(mu_0, Sigma_0, sigma, hogg_x, hogg_y)\n",
    "mu_n = tf.squeeze(mu_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate 50 samples from this posterior. This time however we will not use `SciPy`. We will instead work with TensorFlow Probability distributions. There are many distributions in TFP, including several variants of the Multivariate Normal. For example, there is a class for MVN with diagonal covariance and another class for MVN with full covariance. Obviously, we need the full covariance MVN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raise Exception('Use TensorFlow Probability to construct an MVN distribution with posterior parameters. '\n",
    "                'Store reference in `posterior` variable.')\n",
    "\n",
    "# posterior = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every TFP distribution has a `sample` method that generates samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_samples_1 = posterior.sample(sample_shape = 50, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_posterior_samples(plt.gca(), w_samples_1, hogg_x, hogg_y, sigma, title='Posterior samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling for Bayesian linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Bayesian linear regression model assumes we know the standard deviation of responses (i.e. $\\sigma$). This is a significant limitation. We will now use Importance Sampling to gain an insight into the posterior of a more complex Bayesian linear regression model.\n",
    "\n",
    "To begin with, we will put a prior on $\\sigma$. Our model will therefore be:\n",
    "\n",
    "$$\\large\n",
    "\\begin{aligned}\n",
    "  \\sigma & \\sim \\mathrm{HalfCauchy}\\left(\\mathrm{loc} = 0,\\ \\mathrm{scale} = 10\\right) \\\\\n",
    "  \\mathbf{w} & \\sim N \\left(\\boldsymbol \\mu_0, \\mathbf{\\Sigma}_0 \\right) \\\\\n",
    "  y \\mid \\mathbf{x}, \\mathbf{w}, \\mathbf{\\sigma} & \\sim\n",
    "    N \\left(\\mathbf{w}^\\mathsf{T}\\mathbf{x}, \\sigma^2 \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Let's plot this prior for $\\sigma$. Every TFP distribution implements a `prob` method that returns probability densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "halfCauchy = tfd.HalfCauchy(loc=0., scale=10.)\n",
    "x_range = np.arange(0, 100, 1.)\n",
    "halfCauchy_pdf = halfCauchy.prob(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "plt.plot(x_range, halfCauchy_pdf, marker='', lw=2.0, color='b')\n",
    "plt.xlabel(r'$\\sigma$', fontsize='xx-large')\n",
    "plt.ylabel('density', fontsize='xx-large')\n",
    "plt.title('Density in the HalfCauchy(0, 10) distribution', fontsize='x-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, let's got back to Importance Sampling. From the lecture we know that IS estimate is given by:\n",
    "\n",
    "$$\\large\n",
    "\\begin{aligned}\n",
    "  \\mathbb{E}\\left[ f(\\mathbf{x}) \\right] & \\approx \\frac{Z_q}{Z_p} \\frac{1}{n} \\sum_{i=1}^n\n",
    "    \\frac{\\widetilde{p}(\\mathbf{x}_i)}{\\widetilde{q}(\\mathbf{x}_i)} f(\\mathbf{x}_i) \\\\\n",
    "  \\frac{Z_p}{Z_q} & \\approx \\frac{1}{n} \\sum_{i=1}^n\n",
    "    \\frac{\\widetilde{p}(\\mathbf{x}_i)}{\\widetilde{q}(\\mathbf{x}_i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Combined, this gives:\n",
    "\n",
    "$$\\large\n",
    "  \\mathbb{E}\\left[ f(\\mathbf{x}) \\right] \\approx \\sum_{i=1}^n \\lambda_i f(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "where the weights are:\n",
    "\n",
    "$$\\large\n",
    "  \\lambda_i = \\frac{\\widetilde{p}(\\mathbf{x}_i)\\ /\\ \\widetilde{q}(\\mathbf{x}_i)}\n",
    "                   {\\sum_{j=1}^n \\widetilde{p}(\\mathbf{x}_j)\\ /\\ \\widetilde{q}(\\mathbf{x}_j)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Naive calculation of IS weights would fail due to numerical precision. Probabilities can be very small numbers - imagine $p(D \\mid \\mathbf{w}, \\sigma)$ for a large dataset D! We will instead work in logarithmic domain:\n",
    "\n",
    "$$\\large\n",
    "  s_i := \\log \\frac{\\widetilde{p}(\\mathbf{x}_i)}{\\widetilde{q}(\\mathbf{x}_i)}\n",
    "       = \\log \\widetilde{p}(\\mathbf{x}_i) - \\log \\widetilde{q}(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "Now the weights will be:\n",
    "\n",
    "$$\\large\n",
    "  \\lambda_i = \\frac{\\mathrm{e}^{s_i}}{\\sum_{j=1}^n \\mathrm{e}^{s_j}}\n",
    "$$\n",
    "\n",
    "Note that this is equation for the softmax function! We know how to implement softmax in a numerically stable manner - we did such implementation in the last lab.\n",
    "\n",
    "---\n",
    "\n",
    "Implement `IS_weights` function which:\n",
    "- takes as an input:\n",
    "    - target log-densities (`logp`) with shape: `n_samples`,\n",
    "    - proposal log-densities (`logq`) with shape: `n_samples`,\n",
    "- and returns importance weights.\n",
    "\n",
    "Hint: you may need `tf.reduce_max`, `tf.reduce_sum` and `tf.math.exp` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IS_weights(logp, logq):\n",
    "    raise Exception('Unimplemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance weights allow us to calculate mean and standard deviation of $f(\\mathbf{x})$ with respect to the target probability distribution $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IS_mean_std(f, logp, logq):\n",
    "    '''\n",
    "    Calculate mean and standard devation of f(x) with respect to p(x).\n",
    "    \n",
    "    Args:\n",
    "        f:    Values of f(x), shape: n_samples.\n",
    "        logp: Target log-densities, shape: n_samples.\n",
    "        logq: Proosal log-densities: shape: n_samples.\n",
    "        \n",
    "    Returns:\n",
    "        Mean and standard deviation of f with respect to the target density.\n",
    "    '''\n",
    "    weights = IS_weights(logp, logq)\n",
    "    \n",
    "    if len(f.shape) == 2:\n",
    "        weights = tf.reshape(weights, [-1, 1])\n",
    "    \n",
    "    f_mean = tf.reduce_sum(weights * f, axis=0)\n",
    "    f_var = tf.math.pow(f - f_mean, 2)\n",
    "    f_var = tf.reduce_sum(weights * f_var, axis=0)\n",
    "    \n",
    "    return f_mean.numpy(), tf.math.sqrt(f_var).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we need to define the priors for our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_sigma = tfd.HalfCauchy(loc=0, scale=10)\n",
    "prior_w = tfd.MultivariateNormalFullCovariance(tf.squeeze(mu_0), Sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a proposal distribution and samples from it. We will adopt the prior as the proposal distribution. This should be ok if the target distribution (posterior) is similar to the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proposal_sigma = prior_sigma\n",
    "proposal_w = prior_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "sigma_samples = proposal_sigma.sample(sample_shape = n_samples, seed=1234)\n",
    "w_samples = proposal_w.sample(sample_shape = n_samples, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Finally, to make the above machinery work we need to calculate log-densities. Let's begin with the target log-density. From the Bayes equation we know that the posterior in our Bayesian linear regression model is:\n",
    "\n",
    "$$ \\large\n",
    "p \\left(\\mathbf{w}, \\sigma \\mid D\\right) =\n",
    "  \\frac{1}{Z_p} p\\left(D \\mid \\mathbf{w}, \\sigma\\right) p(\\mathbf{w}) p(\\sigma)\n",
    "$$\n",
    "\n",
    "We need to calculate the posterior log-density up to the normalizing constant:\n",
    "\n",
    "$$ \\large\n",
    "\\log \\widetilde{p} \\left(\\mathbf{w}, \\sigma \\mid D\\right) =\n",
    "  \\log p\\left(D \\mid \\mathbf{w}, \\sigma\\right) + \\log p(\\mathbf{w}) + \\log p(\\sigma)\n",
    "$$\n",
    "\n",
    "Let's begin with the log-likelihood: $\\log p\\left(D \\mid \\mathbf{w}, \\sigma\\right)$. Complete the implementation of `log_likelihood` function following comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(X, Y, w_samples, sigma_samples):\n",
    "    '''\n",
    "    Calculate log-likelihood for each sample in (w_samples, sigma_samples)\n",
    "    \n",
    "    Args:\n",
    "        X:  Explanatory variables, shape n_data_points times 2.\n",
    "        Y:  Responses, shape n_data_points times 1.\n",
    "        w_samples:     Weight samples, shape n_samples times 2.\n",
    "        sigma_samples: Sigma samples, shape n_samples times 1.\n",
    "    '''\n",
    "    \n",
    "    # First we calculate w^T @ x for every sample w and every\n",
    "    # data point x. The output is stored in Z. This tensor\n",
    "    # should have the shape: n_samples times n_data_points.\n",
    "    X, Y = tf.transpose(X), tf.transpose(Y)\n",
    "    Z = w_samples @ X\n",
    "    \n",
    "    # Now we make sure that sigma samples have shape n_samples times 1\n",
    "    sigma_samples = tf.reshape(sigma_samples, [-1, 1])\n",
    "    \n",
    "    # Now we need to create a batch of Gaussian distributions.\n",
    "    # There should be n_samples times n_data_points distributions,\n",
    "    # i.e. one for every sample in (w_samples, sigma_samples) and\n",
    "    # every data point in (X, Y).\n",
    "    #\n",
    "    # Means are stored in Z. Standard deviations are in sigma_samples.\n",
    "    #\n",
    "    raise Exception('Create Gaussian distributions needed to calculate '\n",
    "                    'log-densities. Store the result in `density` variable.')\n",
    "    # density = ???\n",
    "    \n",
    "    # We can now calculate log-densities for our data.\n",
    "    raise Exception('Calculate log-densities for the input data (responses). '\n",
    "                    'Store the result in `logd` variable.')\n",
    "    #\n",
    "    # logd = ???\n",
    "    \n",
    "    # Finally, we need to calculate log-likelihood for each sample\n",
    "    # in (w_samples, sigma_samples)\n",
    "    raise Exception('Calculate per-sample log-likelihood. Store the '\n",
    "                    'result in `ll` variable.')\n",
    "    # ll = ???\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have log-likelihood implemented, we can easily calculate posterior log-densities (up to the normalizing constant). Calculation of proposal log-densities is even simpler.\n",
    "\n",
    "Complete the code below to calculate log-densities (`logp` and `logq`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raise Exception('Calculate posterior log densities (up to the normalizing constant) and store them in `logp`')\n",
    "\n",
    "# logp = ???\n",
    "\n",
    "raise Exception('Calculate proposal log densities and store them in `logq`')\n",
    "\n",
    "# logq = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these many-many steps we  (hopefully) have a working Importance Sampling for a Bayesian linear regression model with unknown $\\sigma$. Let's calculate mean and standard deviation for each parameter.\n",
    "\n",
    "First, noise level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma_mean, sigma_std = IS_mean_std(sigma_samples, logp, logq)\n",
    "print(f'sigma: {sigma_mean:.4} +- {sigma_std:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then regression parameters - we will plot one standard-deviation band together with the data (and estimated noise level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_mean, w_std = IS_mean_std(w_samples, logp, logq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_span = np.array([w_mean + w_std,\n",
    "                   w_mean - w_std])\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_posterior_samples(plt.gca(), w_span, hogg_x, hogg_y, sigma_mean, title='Posterior width')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Random.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
