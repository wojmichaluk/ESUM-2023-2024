{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjLwr7LHju9z"
   },
   "source": [
    "# Gaussian Processes Regresion and its Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y0Y7HX_yju91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "np.random.seed(1234)\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import norm as norm\n",
    "from scipy.stats import uniform as uniform\n",
    "\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gp_samples(ax, x, F, title=None):\n",
    "    '''\n",
    "    Plot samples from a 1D Gaussian process.\n",
    "    \n",
    "    Args:\n",
    "        ax:    Axis for plotting.\n",
    "        x:     x coordinates.\n",
    "        F:     Samples from a Gaussian process,\n",
    "               shape: n_samples times len(x).\n",
    "        title: Plot title.\n",
    "    '''\n",
    "    colors = ['r', 'g', 'b', 'c', 'm']\n",
    "    \n",
    "    if F.ndim == 1:\n",
    "        F = F.reshape(1, -1)\n",
    "        \n",
    "    for y, c in zip(F, colors):\n",
    "        ax.plot(x, y, marker='', lw=2.0, color=c)\n",
    "    \n",
    "    ax.set_xlabel(r'$x$', fontsize='xx-large')\n",
    "    ax.set_ylabel(r'$f(x)$', fontsize='xx-large')\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gpr(ax, V, mu, Sigma, U, f_U, y, sigma, title=None):\n",
    "    '''\n",
    "    Plot Gaussian Process Regression results.\n",
    "    \n",
    "    Args:\n",
    "        ax:    Axis for plotting.\n",
    "        V:     Regression points.\n",
    "               shape m times 1.\n",
    "        mu:    Expected value of f(V),\n",
    "               shape: m times 1.\n",
    "        Sigma: Covariance of f(V),\n",
    "               shape: m times m.\n",
    "        U:     Points at which values of f were observed,\n",
    "               shape: n times 1.\n",
    "        f_U:   True values of f(U),\n",
    "               shape: n times 1.\n",
    "        y:     Noisy observations of f(U),\n",
    "               shape: n times 1.\n",
    "        sigma: Assumed noise level (standard deviation of y).\n",
    "        title: Plot title.\n",
    "    \n",
    "    Note:\n",
    "        Normally, f(U) is not know (observations are noisy). We use it\n",
    "        only to illustrate regression results.\n",
    "    '''\n",
    "    ax.plot(V, mu, marker='', lw=2.0, color='r')\n",
    "    \n",
    "    f_V_2sigma = 2 * np.sqrt(np.diag(Sigma))\n",
    "    \n",
    "    V, mu = V.squeeze(), mu.squeeze()\n",
    "    plt.fill_between(V, mu + f_V_2sigma, mu - f_V_2sigma, alpha=0.3)\n",
    "    \n",
    "    ax.scatter(U, y, s=100, c='g', marker='x', zorder=2)\n",
    "    ax.errorbar(U, f_U[:, 0], sigma, None, marker=\"o\", ls='', capsize=5)\n",
    "\n",
    "    ax.set_xlabel(r'$x$', fontsize='xx-large')\n",
    "    ax.set_ylabel(r'$f(x)$', fontsize='xx-large')\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance functions\n",
    "\n",
    "We begin by defining several covariance functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sqdist(X, Y):\n",
    "    '''\n",
    "    Calculate all-to-all squared distances between points in X and Y.\n",
    "    \n",
    "    Args:\n",
    "        X: Point coordinates,\n",
    "           shape: n times d.\n",
    "        Y: Point coordinates,\n",
    "           shape: m times d.\n",
    "    \n",
    "    Returns:\n",
    "        n times m matrix with squared Euclidean distances.\n",
    "    '''\n",
    "    X2 = np.sum(X * X, axis=1, keepdims=True)\n",
    "    Y2 = np.sum(Y * Y, axis=1, keepdims=True)\n",
    "    \n",
    "    return X2 + Y2.T - 2 * X @ Y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- White noise kernel:\n",
    "\n",
    "    $$\\large\n",
    "    k\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\begin{cases}\n",
    "                                              \\sigma^2 & \\textrm{if } \\mathbf{x} = \\mathbf{y} \\\\\n",
    "                                              0        & \\textrm{if } \\mathbf{x} \\neq \\mathbf{y}\n",
    "                                             \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def white_noise_kernel(X, Y, sigma=1.0):\n",
    "    raise Exception('Implement white_noise_kernel function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian kernel:\n",
    "\n",
    "    $$\\large\n",
    "     k\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "      \\exp\\left(-\\frac{\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|^2}\n",
    "                      {2l^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(X, Y, l=1.0):\n",
    "    raise Exception('Implement gaussian_kernel function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Periodic kernel:\n",
    "\n",
    "    $$\\large\n",
    "     k\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "      \\exp\\left(\n",
    "        -\\frac{2}{l^2}\n",
    "        \\sin^2\\left(\n",
    "          \\frac{\\pi \\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|}{p}\n",
    "        \\right)\n",
    "      \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def periodic_kernel(X, Y, l=1.0, p=1.0):\n",
    "    raise Exception('Implement periodic_kernel function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matérn kernel:\n",
    "\n",
    "    $$\\large\n",
    "    k\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "      \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\n",
    "      \\left(\\sqrt{2\\nu}\n",
    "            \\frac{\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|}{l}\n",
    "      \\right)^\\nu\n",
    "      K_\\nu\\left(\n",
    "        \\sqrt{2\\nu}\n",
    "        \\frac{\\left\\|\\mathbf{x} - \\mathbf{y}\\right\\|}{l}\n",
    "      \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matern_kernel(X, Y, l=1.0, nu=0.5):\n",
    "    D = np.sqrt(sqdist(X, Y))\n",
    "    D[D == 0.0] += np.finfo(float).eps\n",
    "    \n",
    "    Q = np.sqrt(2 * nu) * D / l\n",
    "    \n",
    "    K = Q ** nu\n",
    "    K *= (2 ** (1. - nu)) / scipy.special.gamma(nu)\n",
    "    K *= scipy.special.kv(nu, Q)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets sample some functions from Gaussian processes defined by these covariance functions. Formally, we calculate:\n",
    "\n",
    "$$ \\large\n",
    "  \\begin{aligned}\n",
    "     f & \\sim GP \\left(\\mathbf{0}, k\\right) \\\\\n",
    "     \\mathbf{f_X} & = f\\left(\\mathbf{X}\\right)\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "and then plot: $\\left(\\mathbf{X}, \\mathbf{f_X}\\right)$.\n",
    "\n",
    "Implement `sample_gp` function which takes as an input:\n",
    "\n",
    "- An array `X` of point coordinates, with shape: $n \\times d$.\n",
    "- A covariance function: `kernel`.\n",
    "- Number of samples to draw from the Gaussian process: `n_samples`.\n",
    "- Additional arguments for the covariance function: `kernel_args`.\n",
    "\n",
    "and returns samples from the Gaussian process evaluated at `X`. The result should be a NumPy array with shape: $n\\_samples \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_gp(X, kernel=gaussian_kernel, samples=1, **kernel_args):\n",
    "    raise Exception('Implement sample_gp function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw samples from a 1D Gaussian process. Samples will be evaluated at `n_points` spaced equally between `xmin` and `xmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_points = 200\n",
    "xmin, xmax = -5.0, 5.0\n",
    "\n",
    "X = np.linspace(xmin, xmax, n_points).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from a Gaussian process with white noise covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=white_noise_kernel, samples=3, sigma=1)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'White noise kernel, $\\sigma=1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from a Gaussian process with Gaussian covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=gaussian_kernel, samples=3, l=1)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Gaussan kernel, $l=1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=gaussian_kernel, samples=3, l=3)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Gaussan kernel, $l=3$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from a Gaussian process with periodic covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=periodic_kernel, samples=1, l=1, p=2)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Periodic kernel, $l=3, p=2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from a Gaussian process with Matérn covariance.\n",
    "\n",
    "- with $\\nu = 0.5$ Matérn covariance is equivalent to the so-called *absolute exponential covariance function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=matern_kernel, samples=1, l=1, nu=0.5)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Matérn kernel, $l=1, \\nu=0.5$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with $\\nu = 1.5$ samples from the Gaussian process (with Matérn covariance) are once differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=matern_kernel, samples=1, l=1, nu=1.5)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Matérn kernel, $l=1, \\nu=1.5$ (once differentiable functions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with $\\nu = 2.5$ samples from the Gaussian process (with Matérn covariance) are twice differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sample_gp(X, kernel=matern_kernel, samples=1, l=1, nu=2.5)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gp_samples(plt.gca(), X, f, r'Matérn kernel, $l=1, \\nu=2.5$ (twice differentiable functions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when $\\nu \\to \\infty $ Matérn covariance converges to the Gaussian covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "Now, lets assume we have a set of noisy evaluations of some unknown function $f$:\n",
    "\n",
    "$$\\large\n",
    "\\begin{aligned}\n",
    "  \\mathbf{y} & = f\\left(\\mathbf{X}\\right) + \\boldsymbol\\epsilon \\\\\n",
    "  \\boldsymbol\\epsilon & \\sim N \\left(\\mathbf{0}, \\sigma^2\\mathbf{I}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\mathbf{X} = \\left\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\right\\}$.\n",
    "\n",
    "Our goal is to approximate $f$ using a Gaussian process. That is, we want to learn a posterior distribution over functions that approximate $f$.\n",
    "\n",
    "Implement `gp_regression` function which takes as an input:\n",
    "\n",
    "- An array `V` of point coordinates, with shape: $m \\times d$.<br>\n",
    "  These are the points at which we want to evaluate expected value of $f$.\n",
    "- An array `U` of point coordinates, with shape: $n \\times d$.<br>\n",
    "  These are the points at which we know noisy evaluations of $f$.\n",
    "- An array `y` of real values, with shape: $n \\times 1$.<br>\n",
    "  These are the noisy observations of $f(\\mathbf{U})$.\n",
    "- Assumed noise level: `sigma` (standard deviation of `y`).\n",
    "- A covariance function: `kernel`.\n",
    "- Additional arguments for the covariance function: `kernel_args`.\n",
    "\n",
    "and returns parameters of the posterior distribution over $f(\\mathbf{V})$, i.e.:\n",
    "- mean $\\boldsymbol\\mu_\\mathbf{V}$ with shape $m \\times 1$,\n",
    "- covariance $\\mathbf{\\Sigma}_\\mathbf{V}$ with shape $m \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gp_regression(V, U, y, sigma, kernel=gaussian_kernel, **kernel_args):\n",
    "    raise Exception('Implement gp_regression function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare some noisy obsewrvations of $f(\\mathbf{U})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U   = np.array([-1.5, 0.4, 0.9, 2.0, 3.5]).reshape(-1, 1)  # points where we sample f\n",
    "F_U = np.array([2.0,  0.1, 0.4, 1.8, 0.5]).reshape(-1, 1)  # true (unobserved) values of f\n",
    "\n",
    "sigma = 0.25  # measurement noise\n",
    "\n",
    "y = mvn.rvs(mean=np.squeeze(F_U), cov=sigma).reshape(-1, 1)  # noisy measurements of f(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will approximate $f$ at `N` points spaced equally between `xmin` and `xmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "xmin, xmax = -6.0, 6.0\n",
    "\n",
    "V = np.linspace(xmin, xmax, N).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Gaussian Process Regression to caculate posterior distribution over $f(\\mathbf{V})$.\n",
    "\n",
    "We will plot the expected value of $f(\\mathbf{V})$ and its two standard deviations band. We will also plot (unobserved) true values of $f(\\mathbf{U})$ and noisy observations $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_V, Sigma_V = gp_regression(V, U, y, sigma, kernel=matern_kernel, l=1.0, nu=2.5)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_gpr(plt.gca(), V, mu_V, Sigma_V, U, F_U, y, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example Application: Bayesian Optimization\n",
    "\n",
    "We will now present an interesting application of Gaussian Process Regression: ***Bayesian Optimization***.\n",
    "\n",
    "Imagine we want to minimize some complicated function $f(\\mathbf{x})$. We assume that:\n",
    "1. Evaluating $f$ is very expensive. We can afford to evaluate $f$ only few dozen - perhaps few hundreds - of times.\n",
    "1. We only have noisy measurements of $f(\\mathbf{x})$. Calculating exact value of $f(\\mathbf{x})$ is hard.\n",
    "1. $f$ is a *black-box* function. We can sample values of $f$ and perhaps have some intuitions about it smoothness, periodicity, etc. But other than that, we don't know anything about $f$.\n",
    "\n",
    "Example:\n",
    "- $\\mathbf{x}$ is a vector of training hyper-parameters for a neural network (learning rate, regularization constants, etc.),\n",
    "- $f(\\mathbf{x})$ is the error rate of a neural network trained using hyper-parameters $\\mathbf{x}$.\n",
    "\n",
    "To evaluate $f(\\mathbf{x})$ we need to train a neural network using hyper-parameters $\\mathbf{x}$ and then estimate its rate of errors on some validation dataset. This is expensive. Furthermore, we never get an exact error rate but only is noisy approximation (calculated on some finite validation set).\n",
    "\n",
    "---\n",
    "\n",
    "<u>The basic idea</u> is to approximate $f$ with a cheap-to-evaluate function $f^*$ and then work with this approximation. We will call $f^*$ a *surrogate function*.\n",
    "\n",
    "To approximate $f$ we need samples of its values. But there is one problem: we cannot sample $f$ at will, because this is very expensive! So we need to guess just the *right* points at which we should sample $f$. What could be the *right* point to sample at?\n",
    "- A point $\\mathbf{x}$ where we expect $f$ to be lower that anything we sampled thus far (i.e. value of $f(\\mathbf{x})$ is probably lower than the current minimum).\n",
    "- A point $\\mathbf{x}$ where we do not know much about $f$ (i.e. variance of $f(\\mathbf{x})$ is high),\n",
    "- And many other possibilities...\n",
    "\n",
    "But this leads to another problem: how can we find the *right* point $\\mathbf{x}$ to sample $f(\\mathbf{x})$, when $f$ is an expensive *black-box* function?\n",
    "\n",
    "<u>The second idea</u> is to use the *surrogate function* $f^*$ to select points $\\mathbf{x}$ at which we will sample $f(\\mathbf{x})$. In other words, the *surrogate function* is guiding the exploration of $f$.\n",
    "\n",
    "This gives a following high-level algorithm:\n",
    "1. Let $f^*$ be a probabilistic approximation of $f$, for example a Gaussian process posterior over $f$. This surrogate functions $f^*$ describes our current beliefs about $f$.\n",
    "2. Sample few values of $f$ (e.g. at random points from the domain of $f$) and used them to make an initial approximation $f^*$.\n",
    "3. Use current approximation $f^*$ to select $\\mathbf{x}^*$ - the next point at which we will sample value of $f$.\n",
    "4. Sample y = $f\\left(\\mathbf{x}^*\\right)$ and update $f^*$ with the sampled value: $\\left(\\mathbf{x}^*, y\\right)$.\n",
    "5. Repeat points 3-4 until a stop criterion (e.g. whole computational budget allocated for minimization was used).\n",
    "\n",
    "---\n",
    "\n",
    "There is one important detail left: how exactly do we use $f^*$ to find the *right* point $\\mathbf{x}^*$ at which we should sample $f$?\n",
    "\n",
    "$f^*$ expresses our current belief about $f$ - this is the reason why $f^*$ is a probabilistic approximation of $f$. When selecting $\\mathbf{x}^*$ we should account for:\n",
    "- our expectations about values of $f$, and\n",
    "- our uncertainty about values of $f$.\n",
    "\n",
    "We will encapsulate these two concepts in an *acquisition function* $a\\left(\\mathbf{x}\\right)$ and then select:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\mathbf{x}^* = \\mathop{\\mathrm{arg\\,max}}_\\mathbf{x} a\\left(\\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "Note that we need to maximize the acquisition function. But this is vastly less expensive than optimization of $f$, because  acquisition function works with a cheap-to-evaluate approximation of $f$, namely the surrogate function $f^*$.\n",
    "\n",
    "There are many useful acquisition functions. The simplest one is perhaps the ***probability of improvement***. Let $\\hat{y}$ be the smallest value of $f$ found so far. The probability of improvement is defined as:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "a_\\textrm{PI}\\left(\\mathbf{x}\\right) = P\\left(f(\\mathbf{x}) < \\hat{y}\\right)\n",
    "$$\n",
    "\n",
    "That is, $a_\\textrm{PI}\\left(\\mathbf{x}\\right)$ is a probability that $f(\\mathbf{x})$ will be smaller than any value we've seen so far.\n",
    "\n",
    "Probability of improvement has a closed-form solution under a Gaussian process approximation. Let's define an indicator function:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\mathcal{I}_{f(\\mathbf{x}) < \\hat{y}} = \\begin{cases}\n",
    "                                          1 & \\textrm{if}\\quad  f(\\mathbf{x}) < \\hat{y}, \\\\\n",
    "                                          0 & \\textrm{otherwise.}\n",
    "                                        \\end{cases}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "  a_\\textrm{PI}\\left(\\mathbf{x}\\right) & = P\\left(f(\\mathbf{x}) < \\hat{y}\\right)\n",
    "                                         = \\mathbb{E}\\left[\\mathcal{I}_{f(\\mathbf{x}) < \\hat{y}}\\right] \\\\\n",
    "                                       & = \\int_{-\\infty}^{+\\infty}\n",
    "                                             \\mathcal{I}_{f_\\mathbf{x} < \\hat{y}}\\ \n",
    "                                             p \\left(f_\\mathbf{x} \\mid \\mathbf{y}, U\\right)\n",
    "                                             \\mathrm{d}f_\\mathbf{x} \\\\\n",
    "                                       & = \\int_{-\\infty}^{\\hat{y}}\n",
    "                                             p \\left(f_\\mathbf{x} \\mid \\mathbf{y}, U\\right)\n",
    "                                             \\mathrm{d}f_\\mathbf{x} \\\\\n",
    "                                       & = \\int_{-\\infty}^{\\hat{y}}\n",
    "                                             N \\left(f_\\mathbf{x} \\mid \\mu_{\\mathbf{x}},\n",
    "                                                                       \\sigma_{\\mathbf{x}}^2\\right)\n",
    "                                             \\mathrm{d}f_\\mathbf{x} \\\\\n",
    "                                       & = \\Phi \\left(\\hat{y} \\mid \\mu_{\\mathbf{x}},\n",
    "                                                                   \\sigma_{\\mathbf{x}}^2\\right) \\\\\n",
    "                                       & = \\Phi \\left(\\frac{\\hat{y} - \\mu_{\\mathbf{x}}}\n",
    "                                                           {\\sigma_{\\mathbf{x}}} \\mid 0, 1\\right).           \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{y}$ is the vector of values of $f$ sampled thus far, and $U$ is the set of points at which we sampled $\\mathbf{y}$.\n",
    "- $\\mu_{\\mathbf{x}}$ and $\\sigma_{\\mathbf{x}}$ are the expected value and the standard deviation of $f(\\mathbf{x})$ given the values observed thus far (i.e. given $\\mathbf{y}$ and $U$). Because we use a Gaussian process to approximate $f$, the expected value $\\mu_{\\mathbf{x}}$ and the standard deviation $\\sigma_{\\mathbf{x}}$ are given by the Gaussian process regression:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "f_\\mathbf{x} \\mid \\mathbf{y}, U \\sim N\\left(\\mu_{\\mathbf{x}}, \\sigma_{\\mathbf{x}}^2\\right).\n",
    "$$\n",
    "\n",
    "- $\\Phi$ is the cumulative distribution function of a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's put these concepts into a code!\n",
    "\n",
    "Implement `probability_of_improvement` function which takes as an input:\n",
    "\n",
    "- An array `V` of point coordinates, with shape: $m \\times d$.<br>\n",
    "  These are the points $\\mathbf{x}$ at which we want to evaluate the\n",
    "  probability of improvement $a_\\textrm{PI}\\left(\\mathbf{x}\\right)$.\n",
    "- An array `U` of point coordinates, with shape: $n \\times d$.<br>\n",
    "  These are the points at which we know noisy evaluations of $f$.\n",
    "- An array `y` of real values, with shape: $n \\times 1$.<br>\n",
    "  These are the noisy observations of $f(\\mathbf{U})$.\n",
    "- Assumed noise level: `sigma` (standard deviation of `y`).\n",
    "- A covariance function: `kernel`.\n",
    "- Additional arguments for the covariance function: `kernel_args`.\n",
    "\n",
    "and returns a tuple with:\n",
    "- A NumPy array with probability of improvement values evaluated at points from `V`. The shape of the returned array should be $m \\times 1$.\n",
    "- A NumPy array with $\\boldsymbol \\mu_\\mathbf{V}$, i.e. the expected value of $f\\left(\\mathbf{V}\\right)$. The shape of the returned array should be $m \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability_of_improvement(V, U, y, sigma, kernel, **kernel_args):\n",
    "    raise Exception('Implement probability_of_improvement function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find maximum of the acquisition function. In this example we will use a simple Monte Carlo maximization:\n",
    "- we randomly select `n_samples` points from the domain of $f$,\n",
    "- then evaluate acquisition function at this randomly selected points,\n",
    "- and return minimum from these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_acquisition(aq_f, U, y, sigma, sample_points, \n",
    "                    kernel, **kernel_args):\n",
    "    '''\n",
    "    Maximize acquisition function with a simple Monte Carlo algorithm.\n",
    "    \n",
    "    Args:\n",
    "        aq_f:  A handle to the acquisition function.\n",
    "        U:     Points at which values of f were observed,\n",
    "               shape: n times 1.\n",
    "        y:     Noisy observations of f(U),\n",
    "               shape: n times 1.\n",
    "        sigma: Assumed noise level (standard deviation of y).\n",
    "        sample_points: Handle to a function which selects random points\n",
    "                       from the domain of f.\n",
    "        kernel:        A handle to the covariance function.\n",
    "        kernel_args:   Additional arguments for the covariance function.\n",
    "    \n",
    "    Returns:\n",
    "        Point that maximizes the acquisition function and the expected\n",
    "        value of f at that point.\n",
    "    '''\n",
    "    n_samples = 100\n",
    "    V = sample_points(size=n_samples)\n",
    "    \n",
    "    a, mu_V = aq_f(V, U, y, sigma, kernel=kernel, **kernel_args)\n",
    "    idx = np.argmax(a)\n",
    "    \n",
    "    return V[idx], mu_V[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can finally implement our Bayesian Optimization algorithm.\n",
    "\n",
    "Complete the implementation of `bayes_opt` function following comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_log(step, x, f_x, y, opt_x, opt_y):\n",
    "    '''\n",
    "    A utility function used to display the progress of bayes_opt.\n",
    "    '''\n",
    "    log = 'Step {:3d}\\tnext x: {:5.2f},   expected f: {:5.2f},   measured f: {:5.2f},   ' \\\n",
    "          'opt x: {:5.2f},   opt f: {:5.2f}'\n",
    "    log = log.format(step, x.item(), f_x.item(), y.item(),\n",
    "                     opt_x.item(), opt_y.item())\n",
    "    \n",
    "    print(log)\n",
    "\n",
    "\n",
    "def bayes_opt(f, sample_points, steps=20, kernel=matern_kernel, **kernel_args):\n",
    "    '''\n",
    "    A Bayesian Optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        f: Handle to the minimized function..\n",
    "        sample_points: Handle to a function which selects random points\n",
    "                       from the domain of f.\n",
    "        steps:       number of minimization steps.\n",
    "        kernel:      A handle to the covariance function.\n",
    "        kernel_args: Additional arguments for the covariance function.\n",
    "    \n",
    "    Returns:\n",
    "        Location of the found minimum of f and the value of f observed\n",
    "        at that point.\n",
    "    '''\n",
    "    \n",
    "    # First we initialize the surrogate function with few samples of f.\n",
    "    U = sample_points(size=2)\n",
    "    y = f(U)\n",
    "\n",
    "    # opt_y is the current minimum value of f. \n",
    "    # opt_x is the point where we found opt_y.\n",
    "    idx = np.argmin(y)\n",
    "    opt_y, opt_x = y[idx], U[idx]\n",
    "    \n",
    "    # We need to assume some level of noise in observations of f.\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Now the main optimization loop.\n",
    "    for i in range(steps):\n",
    "        raise Exception('Complete the implementation of bayes_opt function '\n",
    "                        'following comments in the code.')\n",
    "        \n",
    "        # Find:\n",
    "        #   - The point at which we should sample next value of f. Store it in next_x.\n",
    "        #   - The expected value of f at next_x. Store it in f_x.\n",
    "        #\n",
    "        # next_x, f_x = ???\n",
    "        \n",
    "        # Now we evaluate a noisy observation of f at next_x.\n",
    "        next_y = f(next_x)\n",
    "        \n",
    "        # And add it to the known values.\n",
    "        U = np.vstack((U, next_x))\n",
    "        y = np.vstack((y, next_y))\n",
    "        \n",
    "        # We also update opt_y and opt_x if we found a new minimum.\n",
    "        if next_y < opt_y:\n",
    "            opt_y, opt_x = next_y, next_x\n",
    "\n",
    "        # Finally, we log optimization progress.\n",
    "        print_log(i+1, next_x, f_x, next_y, opt_x, opt_y)\n",
    "    \n",
    "    return opt_x.item(), opt_y.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be able to optimize a neural network parameters in this lab...\n",
    "\n",
    "Instead we will use an example 1D function $f$. We imagine it is a multi-dimensional, expensive *black-box* function :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_f(x, noise_level = 0.0):\n",
    "    '''\n",
    "    A function to optimize.\n",
    "    \n",
    "    Args:\n",
    "        x: Points where hard_f should be evaluated.\n",
    "        noise_level: Standard deviation of the Gaussian noise in observations of hard_f.\n",
    "    \n",
    "    Returns:\n",
    "        hard_f(x) + noise\n",
    "    '''\n",
    "    epsilon = norm.rvs(loc=0, scale=noise_level, size=x.shape[0]).reshape(-1, 1)\n",
    "    \n",
    "    f_x = 10. / (1. + np.exp(-x**2))\n",
    "    f_x += 0.5*np.sin(5*np.pi*x) - 8.0\n",
    "    \n",
    "    return f_x + epsilon\n",
    "\n",
    "\n",
    "xmin, xmax = -10.0, 10.0\n",
    "def sample_points(size):\n",
    "    '''\n",
    "    Sample random points from the domain of hard_f.\n",
    "    \n",
    "    Args:\n",
    "        size: Number of points to return.\n",
    "    \n",
    "    Returns:\n",
    "        Random points from the domain of hard_f.\n",
    "    '''\n",
    "    V = uniform.rvs(loc=xmin, scale=xmax-xmin, size=size).reshape(-1, 1)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_points = 500\n",
    "X = np.linspace(xmin, xmax, n_points).reshape(-1, 1)\n",
    "plt.plot(X, hard_f(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And optimize it with 20 steps of `bayes_opt`. This is equivalent to 22 evaluations of $f$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bopt_steps = 20\n",
    "opt_x, opy_y = bayes_opt(hard_f, sample_points, steps=bopt_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this result to a simple Monte Carlo minimization with 22 samples of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = sample_points(size=bopt_steps + 2).reshape(-1, 1)\n",
    "F_V = hard_f(V)\n",
    "\n",
    "idx = np.argmin(F_V)\n",
    "mc_opt_x, mc_opt_y = V[idx].item(), F_V[idx].item()\n",
    "\n",
    "print('Monte carlo search:\\topt x: {:5.2f},   opt f: {:5.2f}'.format(mc_opt_x, mc_opt_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the above cells several times and compare the variance of `bayes_opt` with simple Monte Carlo minimization.\n",
    "\n",
    "### Final remark.\n",
    "\n",
    "Bayesian Optimization is recently becoming very popular in machine learning. It is often used to optimize training of ML models. We just skimmed over this topic and described the basic idea. This is a vast area of research with many algorithms and production-ready software packages.\n",
    "\n",
    "One simple to use but powerfull Bayesian Optimization package is <a href=\"https://scikit-optimize.github.io/\">scikit-optimize</a>."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Random.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
